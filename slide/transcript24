Unknown Speaker  00:00
Analysis that are not mainstream at this point in time. And it also is a clear indication that methodological change in fields that is that are as large as ours is rather a miracle than a sprint. And I was at that with regard to the research method division. The research method division is also one of the few divisions that actually collects feedback from the participants of PWS. And so I've put out here the slide with the QR code for it. Of course, filling out the survey right now probably makes limited sense, so I've put on your table the QR code, and at a later stage, before you leave, please fill out the survey, because these surveys are helpful. When you do multiple iterations, we do get feedback, and the feedback and the forms of what we do down the road, and this workshop was substantially different when we started 15 years ago. Second thing, there's also an award for workshops. Bw so if you're self organizing, or so the research methods division tries to encourage high quality BWS, and the feedback from the participant is one of the dimensions considered in those evaluations. So again, that's on the end. It's easy to do online, on your smartphone, very fast. Results will be available by Sunday. Good. I put something else on your table. On your table, you'll see there's a sheet that says name and email addresses, and that is, if you are interested in the slides and interested in the resource guide that we have put your name on those forms, and my research assistant says I just have to state explicitly, write magically. So block letters are perfectly fine, so that we don't have to search for you on the internet, or somebody has to search on the internet. But again, so if you're interested in that information, please put it down on the sheet. I collect them at the end. Good. We have two hours. Bayesian is a complex methodology, and thereby there is a lot to cover. We try to squeeze as much as we can into the session. So I'm going to lead out with a little bit history and fundamentals and then give track. I'll go over and go a little bit more into details and potential benefits of Bayesian analysis. And then we're going to get a large segment about how to actually execute it. And so Mark kense is going to talk about his experience here. I'm going to talk a little bit about broad priors, and then a loop is going to go deeper into the software applications available for you. The last segment will be about patient analysis and methodological change. Where are we? What's going to come next? And Angie moon is going to talk a little bit about the community set out there and resources that out there. I'm going to talk a little bit back in it up with regard to how I see the future, with regard to why we need patient analysis and how it's going to come around. We have, as you see, 2q and A sessions, so again, in the leak of time for some of your questions and we'll stop. So let's open for the Q and A session, because some of your questions may also be addressed by another presentation. Good without further ado. Again, I have only five minutes, so I'm going to talk a little bit about history and fundamentals concepts of Bayesian analysis. The first thing to recognize Bayesian is not new. It's not new by any means. It's old. It's very old. It's actually older than the frequent statistics, the statistical significance we're using that only came around in the 1925 1930s this goes back centuries with regard to Beijing.

Unknown Speaker  03:47
So this is picture of

Unknown Speaker  03:49
Reverend Tom Bates, who had the insights and wrote these insights, which he never got any credit for during his lifetime, he died before that. We're not even sure this picture is actually him. It's the picture that everybody has and use. But there are people questioning that it is him, because he should be wearing awake and he shouldn't be wearing these kind of clothes in this century. There's also no source for this picture that anybody can identify. So this is probably not Mr. Reference. We know very little about him, but he was genius with regard to the mathematical insights that he had with regard to how, what can I learn from existing data about predicting futurity? So we actually have to thank for Bayes Theorem. It's rather the other class that you probably have heard of, and he actually developed the Bayes theorem as we know it, and he did it about 60 years later, but still, we're a lot further down the road for Beijing. It lingered around for a long time, but it had very limited impact. The big breakthrough with regard to applications was rather a second world war, and several of the military activities were hugely benefited by systematic Bayesian analysis, including moving about the cracking the Germans security communication codes. The Enigma machine was heavily based on Bayesian analysis. So was bombing rates and defending prison bombers. So these systematic government efforts were helpful with regard to developing methodology and developing the tools to actually apply these methodologies. So it's very well established. If you want to know more about the history, I encourage you, there is a very readable book that lays out how this theory wasn't going away. That's my case. Book 2011 and fundamentally, I like this book also because it shows that we, in hindsight, think methodological progress is a straight line, but a lot of that is hindsight rationalization. The actual travel is a theory winding mode, and we should keep that in mind in our own attempts of changing methods. So since Second World War, basic technology, Bayesian statistics is developed and established and legitimized approach of analyzing and learning from data. It is much more used in fields outside of management. And I point out medical, I point out engineering, I have more friends and colleagues in marketing department to use Bayesian than in management. You should also recognize that patient is actually around you more every day, because a lot of the applications with regard to machine learning and AI, I have to be drawing on Bayesian in their update, in their learning processes, they incorporate so patients out there just not on the surface. So what and why? Beijing, don't you already have statistical tools? So here's the logic, and I think this logic is important to understand the contrast what Bayesian does with what statistical significance test does, because that is the fundamental tool we are applying in management. So what's the difference? And one difference is the statistical significance test came about when they weren't computers. It was a great tool to simplify and still make probability statements using tables, and that was part of the success, the simplicity of it, but that simplicity usually also comes with you have to make stronger assumptions, and those assumptions are often then constraining how much you actually going to learn, because the assumptions constrain what You can learn from the data. So I could now easily do an hour session with you on limitation of statistical significance test, and I could tell you about sample size sensitivity and why anything with 10,000 observation system has stars on the other variables, and why that might not be helpful. I could tell you that there are risks when the researchers effort just collecting more data guarantees an outcome, that's problem inherently, with regard to us moving forward and learning about the phenomena, I could tell you that null hypotheses are probably never true, and I know that already before you run data, because any kind of reasonable stuff that I do will probably have some impact, but the impact might be so little that I don't really care about worth the effort. So there's a lot of things to be said about limitation of statistical significance tests, and if you're really interested, I do have that one hour talk on karma. You can look it up from 2018 and it's still full up to date with regard to how we apply significant sets and what the limitations are. But don't get me wrong, there's nothing wrong. It's statistical significance. That's what it says it wants to do. How likely is what I observe been affected by random sampling, that we apply it to stuff, and we don't have random sampling, it's a problem. But we ignore that if we have population data, we still use this other so I'm not, I don't want to, but into that side, I want to take one element of that, of this limitation that's important for the Bayesian side, and that is statistical significance. P values are conditional profitability. You're looking at the probability of observing this data under the assumption that the null hypothesis is true, and then when the confidence interval you're getting does not include zero, it says it's so unlikely that the null hypothesis is true that I can rule that out and throw it on the heap of history. So if I do that, this has served a purpose, a probability signal that allowed me to say, like, now this null hypothesis is invalid, but keep in mind, what happens the p value served a purpose. Now I'm pointing out the assumption under which I calculated the p value. That p value is not a probability statement. It's good for anything else anymore. While in our research, implicitly or explicitly, we're kind of reading into it, the probability of that our hypothesis of interest is true and it has no value for that. It doesn't even have value for what's the probability for your null hypothesis is being told because you calculated it under the assumption the null hypothesis isn't true. So you can't use that. And that's not Saturday Jacob Cohen has out there. That's the inverse probability fallacy that we implicitly explicitly free to often engage in arguments. But that's in the end, what David calls it answers a question. And it's not the question we're really, really interested in. We want a probability statement, not about the null hypothesis one, but the assumption of the hypothesis is true. We want a probability statement about the effect that we're interested in, that we're hypothesizing, and the P values cannot provide that. Here's the good news. Bayesian can with all its limitations, that's exactly what is the probability of height h1 or hypothesis being true under the assumption of this data that we observe now it makes other, sometimes other limitations, and at least tries to answer the question that we realize, and that seems to be a promising and deprecating agent, than answering a question that has created the value class to develop theories, if you want to read more about that, I, like most these two papers in the American Statistical the American Statistician, um, they're both editorials. And the first one is pretty much how all these people using have to do as a community to stop other people from misinterpreting p values. And the second one is about, well, if we want them to stop, we may have to tell them what else they should do. And so the second one is all about alternatives, and there's multiple proposals and things you can do. Bayesian is one of them. So again, this is one suggestion from moving forward, but one highly legitimized through the American Statistical Association. So conclusions Bayesian analysis are well established. They're not new. There are tools that's available to us that other fields have already used. Bayesian analysis can be applied to virtually any research question, any statistical problem, any empirical context, and this Bayesian posterior distribution that is estimated provides us information about the size of the effect and the frequency distribution of that effect, so the probability, the uncertainty associated with and that is a answering the question we really need. This is a better base to build quantified theories. So that's what I have in short nick of time for you on the intro, and I'll hand it over to Bill. Bill. Hand it over to David. And we

Unknown Speaker  13:10
missed bill. We missed

Unknown Speaker  13:31
great.

Unknown Speaker  13:35
Okay, well, my so my job is to try and convince you that basic statistics in a publishing context, really helps us to encourage and encourage us to ask really important questions that we have a very hard time even asking using traditional, traditional statistics, the tradition. I don't like using the term frequentism, because that doesn't mean anything traditional, though. That means something. What we traditionally do, what we've all been trained to do, is develop things like testing models, testing the difference between models by using an F test. So we typically look at the difference between the sum of squares explained, but that requires it to be nested model. And there are a whole bunch of other problems with with doing that, that that we get to avoid using a Bayesian approach. So the first thing I want to do is suggest that what we can do is really help us understand whether or not one model or another, or perhaps even a third, is a better overall explanation for our observations. And I want to just provide two examples that really very different applications of this overall concept that that Bayesian statistics allows us to do. So first I'm going to provide very simple example. We got two models that might be that we have two model, competing models that are not nested, that are not necessary, two different explanations of the same same data, and we want to find out which one is right. And so what we do is we do this base factor analysis, and we have model one compared condition on the data versus model two. And what we're doing, really is comparing them and saying, basically, what is the probability the model one is right? That's a question that we can easily answer with a Bayesian statistical approach. And I want to give you an example of where they did that. Very bossy, a very famous physicist who's spilled over into ecological science and also innovational science, and at least into network analysis, claims that, in fact, all networks can be described and with a power law distribution. He never saw a power law he didn't like. And basically what he showed is, and this is just one example where he looked at communication patterns, which were the delay and people answer email, faculty answering email from each other. And he found that, as he as he observed, that the the distribution of time that it takes to reply to faculty when faculty send each other emails ranges from something like one second up to like several hours, up to longer than that, this little log scale, and the fact that that log log looks Linear, leads him to the condition, leads him to the conclusion this is a power law distribution and power laws rule. As far as the robust he's concerned, he's made a lot of hate with this very simple observation, along comes somebody else, though Stouffer, Mel green and Amaral, who are networkers of also a physics bent and and they asked the question whether or not this is written Vera bosses interpretation is true. And they said they've noted that a log normal distribution looks very, very similar to a power law distribution. And so, and they've been also fit the data if you're using a statistical approach. But which one really is better one to use? And what they said is, we can answer that question by doing a Bayesian analysis, doing a Bayesian factor analysis. And so they measured the the ratio of the difference, the ratio of these two models, explaining explaining the data, and they came up with, they, they did this kind of traditional Bayesian analysis. The whole result is this picture. One of the first things Bayesians do is they are much more compelling, I think, with pictures than simply numbers. They don't give you these boring tails, they give you truth. Let me explain this picture, because this summarizes the whole result. Over here we have the that probability of the probability that garabasi is right probability that the power law distribution model is the correct interpretation, as opposed to that probability runs from one all the way down to one in a million goes down below that. All right, so now what we see here on this axis is, well, how many observations do I get to include in my test of these two things, it turns out that if you only have one observation, bear boss, he looks like he's right. If you have 10 observations, bear boss, he looks like he's still sort of right. Maybe, if you have 100 observations, though, you get more pessimistic. And if you have 1000 observations, matter of fact, around 800 observations, all of a sudden your faith in Barabasi falls off. Okay, there are two edges here. So the probability that, in fact, Barabasi looks like he's right in this, in this analysis, really, you start discount, you start saying that can't possibly be right. There's only one chance in 100 he's right. When we have 800 observations and we have 900 you know we're talking we're talking about one in a million, and even beyond that, the second thing I want to point out is they plot two different curves here, which look look very, very similar, this red curve and the black curve. And turns out, though, that they really make very different assumptions. The red curve is the idea that we're going to give a prior that, you know, it's 5050, very boss is right. We have a prior that he's right 50% of the time. And we have a, perhaps a and the black one is, is saying that bear Boss, he is right 95% we're going to give bear bossy a huge prior advantage, if, if, by assuming that he's right with a probability of point nine five, that's our prior he's right because he's big, he's powerful, he's published a lot, he's been cited over 100,000 times. He must be right, so we're going to give him a point nine five. It turns out, you look at the difference between the red line here and that black line, and there's no difference. In other words, what this graph also shows? First of all, it shows you how information you need in order to really distinguish this like a power law, power analysis. But in addition to that, it shows you how insensitive the results are to your priors, which is always a big criticism. I love this analysis because it shows you that, in fact, I don't care what priors you could put bear Boss, he is right with one of them, that he's wrong only one out of a million times, and you're still going to get the same curve. That shows you how insensitive the result on this analysis is to that prayer. That's an important that's an important demonstration. I'm going to compare that with another one. The other one is very simply, a question, completely different question asked by a bunch of ecologists, and also a Bayesian bunch of Bayesians, including J coding, and they're going to ask the question whether or not our analysis of some datas allows us To determine, you know whether our assumptions underlying that analysis is really determining our result. So their example is going to be, is climate change observed in an increase in the frequency and strength of hurricanes? Or if that's 50 years? That's an important question, not an organizational one, but it's okay. Sometimes we are interested in other things. J is a very, very prominent

Unknown Speaker  22:23
Bayesian that got interest in this problem. All right, what they showed, what they found was that, okay, if you look at the trends over time, it looks like the trends are in cycles, but over time, there does seem to be an increase in the total number of cyclones, because they're looking at the Pacific, not not the Atlantic. And but it turns out that over time, the technology for detecting cyclones really improved. Used to be they only use ships. Ships would come back and say, Boy, we had a cyclone while we were crossing the Pacific, and now that we have satellites and everything, but every technology has error rates, missing rates of different kinds. And people said, well, that is going to have a big effect. Because if we see more, simply, because we see more, not because there are more, we have to take that into account. So there are three different approaches to this missing data problem, depending on whether the the cyclone itself lasted only six hours at a category one or 12 hours, or, you know, 10 days makes a big difference whether you can detect it. If it's a 10 day, it doesn't matter whether you're going to you're going to catch it anyway, because it's 10 days long. And some ship is going to even back in the 1880s however, if you have shorter duration ones, you're going to miss them back here and miss them here. And these two fellow periods here are because of world war. World War, you know, world wars tend to interrupt our ability to collect data, so they found far fewer cyclones because, look, this is complicated. This is complicated. And they said, Well, let's take a look at different ways to assume what this is. And they had model one. They had model one of the, you know, that detected because of what I just showed you, and model two still back in the old days. If it's a 10 day cyclone, you can get it but but much less ability to detect it using that old technology. Model three says, wasn't that bad, and they okay. So they said, Well, let's take a look at the number missed, and they put that into each of the assumptions. And in model one, they said, we only miss maybe, you know, something like 10 per per year. And then model two, well, we might with a higher frequency for a longer period of time. Or although they look kind of they don't look very different. That is, the assumptions under don't look very different. Model three kind of doing this more, but the rates weren't that different. And what they found, though, is, depending on the model that the Bayesian and posterior interpretation was that, in fact, we believe that the increase was 25% per year. That's the best estimate. Or using the second model, we believe that it would increase 11% per year. And under the third set of assumptions, we believe it's decreased. We actually believe it's decreased because we've just missing them early on. And what they did was they said, let's look at the posteriors of all these and it turns out, the probability that it's increasing under the basic Notice also that the distribution, the prior distribution, is really broad. It's not influencing the results here using a prior. This a very confused prior, but that the assumptions underlying shift this curve of the posterior just slightly to the left, such that and each set of assumptions which weren't that different from one another, all of which were reasonable. So their conclusion is the results appear to be highly sensitive to our assumptions going in. It. Nobody else bothers checking them. Bayesian allows you to actually check that our assumptions affecting our results. There is no sense, they said, in using these data, given how sensitive this is to answering this question, therefore, what we need to do is move on and get more or better evidence. Don't argue about what this is, because it's too sensitive to our assumptions basis.

Unknown Speaker  26:56
Allow you to look at that question.

Unknown Speaker  27:00
Both of these, I think, are wonderful examples with the power of what you can do to look at how sensitive your stuff is to the assumptions to your priors. And we need to do that more and regular, traditional statistics. They just ignore that problem, no questions, because

Unknown Speaker  27:26
later, later, sorry for those in the back. Could you come up front? People are coming in and there's no room, sorry.

Unknown Speaker  27:35
Or could you come up friends? So while I'm setting this up, I want to say like, thank you.

Unknown Speaker  27:49
And the one big addition, what David was just describing you, also showed all the time there were probability statements of the light of this model being better than another, and these are actual probability statements about the hypothesis. This the p value. Can't give you that comparison. You throw out models, but at the null half model, that's all you throw out. This here allows quantified comparison and quantified conclusions on it. The second thing that he, of course, sets me up perfectly With is that the, sorry, my computer goes wild. Okay?

Unknown Speaker  28:46
Hold on. David does that for me every year, and I'm trying to convince them to give me the slides in advance so I can integrate them. And he likes his pen a lot.

Unknown Speaker  29:02
Great.

Unknown Speaker  29:18
Yes, all right, and where did I put the clicker? That's the only other thing we are operating okay. So back to the point. He pointed out that a big part of the flexibility that Bayesian is providing is that we have these assumptions called priors that come in. But this might confuse you, because you may have never heard about priors. Our again, statistics, significance doesn't do anything like that. So now I'm going to talk a little bit to get you familiar with what these priors are and what functions they serve, so that you can also then see where and how David and his examples was building on that capability. So all the specification of priors. So what are priors? Are the distribution, our expected distribution of the effect that we're analyzed. So they are our before we collect data, expectation of how a variable is affecting the outcome variable. What functions do they serve? Well, number one, they are required by Bayesian analysis, so we can't avoid that. So you have to specify the prior for each variable in the models, because the model itself will use the actual data to update the priors. And as David nicely points out, if we, for example, have a lot of data, we can rely on the data the prior will not make a difference. We know it's data dominant, but we can't take that for granted that the second example was shown, so it allows us a deeper analysis, so priors can influence results. And you may say that, Oh, that's dangerous, but think of it the other way around. If we know something about the phenomenon, why ignore that prior in and if we have a good prior we can work with much smaller samples. So I had friends living across the street who had worked for NASA. They were doing vision analysis with samples of four or six observations, because the seventh observation of what they wanted to do was very expensive, and you could learn something from the six even vaguely, and it was worth the time and effort. So think of Asian also opening up doors with regard to stop where events we're not investigating right now or we're investigating very quickly, so can influence results that that's not necessarily a problem, as long as we're doing this system again in a proper way. So thereby, priors open the door with regard to incorporating what we already know into the analysis Beyond the Data. Good specify that front priors can be more specific and priors can be more vague. So that's what I'm saying down there. The End Date means they're diffused. You have a central tendency, and then you have a distribution of a central tendency. And the most diffused priors are uniform priors where you assume each outcome is equal enough. So how do we get to these priors in a systematic way? And that's what I want to talk a little bit about, but limited time. So we'll show you five approaches that could lead to meaningful priors, and we'll talk about each of them in turn, list them up up front. So let's start with uniform priors. Uniform priors are used when we know nothing or close to nothing about a phenomenon. We have no idea what the distribution is going to look like. And then the alternative would be give each outcome the same probability. There's also something called weakly so that that doesn't weakly uniform priors and formative priors. This is pretty much a central tendency, but with large, large variance. Currently, you should also recognize the software that Anoop is going to talk about later, most of them has as a default uniform priors in there. So if you don't do anything, you'll have your running uniform prior. There are limitations for the uniform priors because there are strong assumptions. You're assuming each outcome is equally likely, but that's something that we really, really find actually we're investigating because we don't believe that that is actually true. So you're making a strong assumption, and thereby you're also potentially constraining the outcome. If you have, for example, some notion that the generators process behind the data would rather suggest not a flat distribution, but a Poisson gamma distribution, you're ignoring that, even though, by the generative mechanism of what is happening out there, this is highly unlikely, and you are constraining your analysis with its predictive power by your input. So uniform priors are strong assumptions, and thereby you should be aware that you put a strong assumption to be announced. Recommendations use when you really know very little, then what alternatives do we have? Even when you do that, prior should always be justified. So explain why you think we don't know anything, or what is available is something you don't trust, and there may be good reasons to discuss even a prior research can use uniform priors to pass data dominance. That's kind of what David did in the first example. You just put in different assumptions, three different assumptions, and they showed you that it doesn't make a difference. I don't have to worry about priors, because my data speaks by itself. They can be a useful benchmark, even when you have smaller samples. And they do make a difference, because you can quantify To what degree is the prior the gamma distribution that I put in there, what amount of effect does it have on the outcome versus the uniform prior that I would put in. And thereby I get a judge, how much do I trust in that? And I get a quantification for the relative impact. That's a much better basis for a debate to have with regard to what really do we know about the phenomenon that was wonderful, uniform or price, and it's frequently used. And my other arguments is that that is easy and simple, and when you have data dominance, that works, but at the same time, we're not opening up that door of what else we could study with regard to more efficiently, when we use priors and we know something and when we can build on what we already know. And if you think of our fields, we're doing multiple studies in order to iteratively get better at predicting and understanding the phenomenon. So our prior research should inform what priors make more sense, but there are limitations, especially in our field. We don't do applications. Applications would be really good input. We don't do that. We do studies that are kind of related. But how are they different? How much are they different? That would require discounting with regard to creating the priors. We have questionable research practices. We have replication crisis in the field that could lead you to conclude that you put less trust or no trust into the prior studies that are done. But that shouldn't be the default. The default in our field, moving forward, should be, of course, that we have prior research and informs, and we do have some even systematic ways to do that, even if you don't trust all studies, there will be, still some studies that you think tell you something, and we can hone in on those, and you can argue for those studies. We do have people who do meta analysis, and a meta analysis might be a great if it's a good meta analysis, if it's a meta analysis that has a lot of apples and oranges in there, but very different studies may not be as good, but the but this really the default, maybe, has built in itself into the method of approach to incorporate prior knowledge, and that is a real opportunity with regard to progress over time, over study. So that's on the other option. But you also see in other fields, is continuous theory. If you have really good, quantified theories with predictive power, that's where our problem sets in. We have theories, but we have theories that are a lot about there is an effect and there isn't an effect, and that's what we have, but not. Where are the boundary conditions for this and what's the size of the effect and the distribution of the effect that's partially related to the methods that we use. If we would use more variation, we would also probably get more predictive and more quantified theories. Even here, generative mechanisms underneath the effect that you're observing can tell you something about the functional form, and we can at least put that into as a starting point, and if you have a gamma small distribution there, that's substantial difference with regard to a normal distribution or a flat uniform. So theories and in the future in another field, sometimes physics very helpful. Sequential progress is also something we don't do much, but we see that happening more partially as we see more big data and more continuous data collection. So in a sequence of studies, you can use the results from the earlier iteration that you did as the prior for the next, and you update what you found in the last year's data with this year's data. And that's a very powerful can be a very powerful source for priorities, good. And then another thing we stubborn study phenomenon that typically are managed. There are people actually trying and deal with these issues every day, and they may have some insights they may not know all about it. Maybe some cases where they have no clue what they're really guessing, but in others, they may have some input, and that knowledge may be valuable with regard to as a starting point to interpret your data.

Unknown Speaker  39:16
This also has the advantage that these managers or practitioners that you would be asking are not the researchers, so the prior is not influenced by the researchers themselves, but by an outside source, and potentially a source with other insights. So triangulation items come into play. However, asking practitioners of what their expectations are with regard to outcomes of the actions of organizations has its own challenges. Number one, you got to get these decision makers. You got to get access to them. Number two, we know individuals have perception biases. University Hanuman all they're well established, but we also know a little bit of what we know about the biases. We can also start working around those bias so at least take them into a gap. Still, there is a huge literature showing that people have difficulty translating their gut feeling into nice probability distributions. But we don't have to reinvent the wheel here. There's an entire literature in forecasting who does nothing but developing elicitation tools and recommendation here is find experience and train subjects in it. There is ways how to do that. There are graphical ways how you can use sliders and help individuals to kind of formulate and communicate what they're thinking. And there is a verging advanced of computer based elicitation methods that barely are used in our field, even though we're dealing with managers, or should be dealing with managers. And side note, I even see the advantage if we start doing some of that, we're interacting more with managers in general, which is probably would be helpful for us as a field in general, just to get that input and their perspective. So first conclusion, buyers are an important opportunity for reflective empirical analysis. They create flexibility that can be used for a deeper investigation focuses researchers on the distribution of effects, not a p value effect size and the distribution of it, it encourages the explicit justification of why you use these buyers get that constantly asked by your reviewers, and the transparency protects against arbitrary priors because you have to justify them. And if nothing else, somebody will say like, well, I don't believe in that prior. What are your results with my prior? We can run that and see it, and we can then start a discussion about which ones are. That's a much more substantive discussion. Second, there are established approaches how we can get to them. So it's not that there are best practices that we can use. So it's not just researchers in their own little chamber, thinking of priors, there are simple ways to incorporate what we or others know about the phenomenon, and if we have multiple approaches, we can compare those approaches. We can sometimes combine these approaches, and depending on the phenomenon, get a better predictive model. They enable sensitivity analysis. Again, David showed example on that already, data dominance is great when you have it, but at the same time, when you don't the phenomenon they don't offer plus data dominance. Yes, we have samples of 10,000 observations, but it's companies from 2500 industries. And in 2030 different countries, and we're just assuming they're all the same, and our effect, it doesn't matter, and we can't control for that. So we are very heterogeneous sample with all kinds of spurious effects. If I can study more homogeneous samples that are smaller, that's another way to probe and it might be having higher internal validity than a lot of the studies were running. So trustworthy priors, if they can be developed, they can allow us to study phenomena more efficiently on smaller samples and still be meaningful enzymes. So Bayesian analysis fosters specific progress by providing a systematic way and a required way to integrate what we already know in our analysis, thereby tires are an opportunity at all, something we can flexibly use to understand more about and learn more from our empirical data. If you interpret our data that way, we get accurate quantified probability statements about the effects that we're interested in, and thereby fires are not the enemy. They're not familiar with them, but at the same time, they're actually a very advantageous and key feature of Bayesian analysis that is used by this part of why Bayesian analysis is interesting and powerful good. So that's where I'm going to stop and hand it over to with regard to more execution oriented and management examples.

Unknown Speaker  44:15
Thank you very much. Well, given what's been said so far, I want to start today by sharing with you some of my first exposure statistics. I got my degree at Texas A and M. I had a friend there who studied statistics at the time, and then years later, we both ended up saying University, and one day he asked me what I was doing, what I was working on. I started to explain it to him. He said, You know, you really ought to try basic statistics to analyze that problem. So we already had most of our data collected. And I said, we've already got the data gathered, it assuming we were going to use traditional or frequent statistics, doesn't matter, that we can make this work. And that's where I first started, down the basing pen at about that same time, in a conversation with one of my other colleagues who got his degree in economics at a well known Western University. I told him I was using Bayesian, and he laughed, and he said, In my PhD program, our exposure to Bayesian was basically this, if you run a model with frequentist or traditional statistics and you don't like your results, then try basic and what I found over the ensuing years is that, unfortunately, a lot of people think that, and it couldn't be further from the truth. And so I tell you that right up front to indicate that it's an uphill battle. But even though it's an uphill battle to try to get something published using Bayesian statistics in a Management Journal, it never ceases to amaze me that we look around and see it used so effectively in other areas. All of that is to say, I'm glad you're here today. I'm glad for your interest, and we all hope that in time, basing statistics will become more widely accepted. And I if you can look at some of the slides that were presented today by both David and Andreas. There are some huge implications here. What you can say with a Bayesian analysis is fundamentally different from what you can actually say from a frequent analysis. The problem is that over time, especially in management, we have tried to say the things draw the conclusions from a frequent approach that you really can say with a Bayesian approach. And I think that's led us down a very unfortunate path. When we see frequentness analysis, we're drawing conclusions that technically or statistically speaking, you really cannot draw that conclusion from the results that we see. And just one quick example of that, and I see this, especially in acutely abnormal return states with a previous approach. When you get a point estimate and a confidence interval, as was explained earlier, what that really means is, if we were to draw an infinite number of samples, 95% of the time, estimate would fall within that context. That's that's the conclusion you can really draw. What you cannot draw is that that that any one point estimate would be any more likely within that confidence interval than any other point, that becomes a huge problem, especially when that confidence interval stands. There are so many studies that say, oh, yeah, this, this has a positive effect on performance. Well, it didn't that sample, but if you were to draw other samples, it might be that is a huge problem, and Bayesian has a very good answer for that kind of problem. All right, so here's what we started out. We wondered about how certain strategic actions that a firm could take affect performance. So we We gathered our performance data, quarterly returns, and we looked at all these strategic actions. And early on, we also had the CEO in there, when we used this to do some studies on CEOs. So the basic kind of things that you might look to with any other kind of methodology. Now, if we compare a frequentist approach to a Bayesian approach. Here are, are some of the main ideas. And I've already talked about this idea down here, and you've heard mentioned today about posterior distribution, that's really the main output that we're concerned with a Bayesian approach, because that tells us, here is the likelihood of that effect, the effect of, in this case, any one of those strategic actions on performance. And here's how that distribution looks, how that probability would look. Another thing I would point out here is that with the Bayesian approach, you're using a hierarchical Bayesian model, you automatically model endogeneity. It's it's just not the huge problem that it is other places. And that comes from the fact that in a Bayesian model that we are assuming in this case, for example, that these actions are taking place within a firm, that firm is within an industry, and all of that is modeled in your basic model, and as complex as Bayesian statistics are often described as being to model that is actually very simple and straightforward. It's surprisingly simple to create a model that would handle extreme complexity if you are forced to use a frequency approach. Okay, so here again, if you look at these models, these look very similar. The big difference is the eye here, and this is where we get to this idea here, by using a Bayesian approach. We're we're recognizing that the action a that happens within a firm, that happens within an industry, that can all be part of the model. So we're handling endogeneity right out of the gates. The other thing this allows us to do is to create a posterior distribution for every firm in our sample. We can look at it in the aggregate, short, just like we would with a frequent approach, but depending on what we're looking at. We can develop that set of results by firm, by industry, by CEO, or what other whatever unit analysis we might be interested in, and we can look at those results in a very meaningful way. Okay, so here is a comparison between an OLS model cool regression. Here are the actions we looked at, and again, performance here as market returns and if we were to use this traditional approach. And you can tell I'm probably running maybe crazy by using three traditional we would come to this conclusion. We would say, this is restructuring, this is organizational restructuring. This is key personnel changes. We would say these are the only three that have a p value low enough that we would consider them to be statistically significant. So in a traditional model, that's where we would get. And you'll see here we have an arc R squared that's very low on this data set. Everything else we would come to the conclusion, it just doesn't matter, that there's no effect. The other thing we would be doing with this kind of approach is we'd be saying every firm in our sample is basically the same, that there are no real differences, and there are no differences in industry. This is just average across everything we've looked at here, okay

Unknown Speaker  54:05
with the Bayesian approach for each one of those actions, we would have distribution like this. Now, if you look at new product introduction, right here, new product introduction, you'll see that distribution is fairly tight right away. That's telling us that it's just as likely to be positive as it is to be negative. In that sense, it's fairly consistent with the frequent approach. It's telling us. We can't really say that, and this is in the Angie for all of the firms in the sample, we would draw that conclusion. But let's look here at financial restructuring that's centered about where we would expect given the results that we saw with the frequent approach. So is financial, organizational restructuring and the personnel changes. They're positive. You know, it looks like it's center positive region, but there's a lot of that posterior distribution that's less than zero. Okay, now, here is another way that the data can be presented with these interval estimates, and it tells a very similar story to the distribution in terms of how wide it is and where. So you can imagine, from this kind of data, we can create a table, and this included point estimates that would be quite comparable to a frequent but again, these are telling us something very different. These are actual probabilities. The way that we would interpret this in this study is that, for financial restructuring, for example, we would say, on average, across all the firms here, if a firm were to engage in yet another financial restructuring, and I'll show you this in just a second, there would be a point nine, two probability that that would have a positive effect on firm performance for whatever firm undertook. That is a probability based on the data as described Okay, and here that is, you can do a fairly quick calculation. In fact, the software will just tell you, you ask it, it can generate this in the aggregate across the whole sample, or any of the firms or industries, whatever you happen to be interested in. If we tell you this, that's how that would be interpreted. So with this methodology, instead of looking at a p value, we would just show this and let the reader draw their conclusion. Does this support a hypothesis or does this contradict? Okay, and this is here just to show that in this particular sample, we have 42 firms, small number of firms, because we're looking at this longitudinally. We're still working adding to this database, and at this point, the database we're currently using has like 46 firms, but has four years of data. Okay, the story that this tells this is for financial restructuring, if every firm is actually the same. Many of the firms are in positive territory, but some are in negative territory. Many of the firms interval spans zero, just as we would expect that's that's reality. Okay, here is just a quick snippet of the code. And I I include this here. And again, it's a snippet. It's not the whole thing. But to show you that this is very simple, as we were sitting here getting ready today, a new said, I wonder if I could get to use GPT.

Unknown Speaker  58:46
I wonder if I could get it to write these code. So given a couple of quick prompts, but no one's

Unknown Speaker  58:54
part of the reason. The point here is that this is very doable. This is very doable, and in different SATs packages these days, it's getting even more easy. Now, in conclusion, here what you see there again, I would come back to this. It's that difference between actual probabilities with a frequentist approach. We are very tempted to say probability. It is not probability. Here. It is actual probability, one of the big differences, another of the big differences is with the Bayesian approach. You're looking at the probability based on this data. It's not based on sampling from a crop population. It's based on this data. Here are the proper buildings. And that idea that I expressed earlier, that my colleague made in statistics. What he was saying is the belief was, when you set your priors, you determine the results, and all you got to do is set the right prior.

Unknown Speaker  1:00:25
That is simply not true, depending on the complexity of the model, once you get it on data the prior previously. But quick word about priors before I completely care. It's that idea of being able to update. Probably heard the term easy and updating in clinical drug trials.

Unknown Speaker  1:00:57
In trial one, they'll get some results, and once they know those results, that helps inform the priors for trial two and trial three, and if they have to go on to additional trials each time, the law is better and better at predicting and showing what the data are actually telling, if you've ever shot on the panel statistics or work in the background, trying to decide what to show you next, trying to decide What kind of a deal you are most responsible, and it's doing that by borrowing data from the other millions of customers who are shopping on that site, combined with your own behavior on that site, so that they come To very accurate models. And the more you shop there, the better they are. And some Greek people are wind down by that. Some people really agree. My experience is sort of people my age see that. Oh, how can they know so much about young people? Look at this saves me so much time. So in conclusion, Bayesian statistics are powerful. They're getting increasingly simple and straightforward to use, and by and large, they help answer the questions that we are really asking or that we ought to be asking. And this study that I just described started out as they looked at the resource using resource based view theory. And when I explained that to my colleague, who was pro basing, he said, Hey, that theory is a very good fit with Bayesian statistics. What you're assuming sounds like the resource based view is kind of a theory of outliers. But he said it, and he was right. He said Bayesian will allow you to model that kind of so in the end, Bayesian statistics make a lot of stance because there are no of the

Unknown Speaker  1:04:00
harmony theories we tend to use. Thanks Mark, everybody.

Unknown Speaker  1:04:02
So my I guess you've heard a lot about why ladies and statistics is extremely valuable for us to do as important questions that we're interested in answering. And I thought was maybe I'll talk a little bit about the actual implementation, right? So when we think about a Bayesian model, how do you structure it? What are some of the key parameters that you should be thinking about, and what will be some helpful tips in terms of where they are. When I think about this, I go back to these when I was a graduate student two years ago to the Bayesian statistics class, and I was like, I would never want to do this, ever. It was horrible, very, very hard to kind of understand what was going on at that point. And I think now we have so much more resources. That was a time when I think very few management scholars were really interested in this. And reason why I took the basin statistics class was because of another friend of mine who's in marketing and who's done multiple times the same program, Jeff Dodson, and he said, Let's go do that. All right, let's try it. But just saying that, 20 years, there's been so much advances on this particular area. These days, it's much easier for us to do with amazing study. So we'll talk a little bit about what is the fundamentals of Bayesian computation? What are some key elements that go into it? And then I'll also talk about some software tools that are available for us right now to conduct Bayesian inference. So the building blocks we've heard you've seen this multiple presentations now before David talked about it, Andreas talked about it. Mark, here's the fundamental idea behind any kind of basic study. You've got three components, right? You've got your left hand side. It's basically going to be your posterior distribution. That's what you want to study on. All this is not determined by statistics. It's determined by your theory. Whatever topic it is that you're studying, you've got to kind of translate that into this particular language. So you want to learn about some parameter, theta, given the data that you have. So in Mark's case, he was trying to study what would be the outcomes when you're looking at strategic action, something like that. So that's what you're interested in setting. And then you have you need two components to work with. You need to have data which you typically would have if you're doing frequent studies. You also need data there. The big thing is going to be your assumptions on the prior distribution. How you specify that prior distribution can actually make a big difference in terms of how the results come out. From a publication standpoint, it's always been a form, right? So people, when you submit papers for review, they'll always say, Well, how strong is that prior dominating the results? That's that's always been one of the pushbacks against the reason based in scholarship in general, but I think that's changed, and these days it's much more accepted from that perspective. So the material basically will reflect our revised beliefs. What do we learn from the data based on this new information that you have? What's based on our prior and new information that we have? So let me give you a simple example of so let's assume that your data is drawn from a solution with a and then let's assume that your parameters that you're looking at also are drawn from a normal distribution. So this is a standard case where we have a normal normal distribution, a normal normal framework, you would easily derive mathematically, what would be the distribution for the posterior in this particular case, this is basically what we would call the closed form solutions. Or the reason why you can actually to write this mathematically is because you're using properties of the normal distribution in the past this this was kind of like the building block, the starting idea right the conjugate distributions, where you can find some if you can cleverly find certain distributions that work together, you can always derive a closed form solution, but the power of Bayesian is well beyond it. So what if you can't find that conjugated solution? What if you cannot mathematically solve the close together distribution? And this is where I think a lot of modern Bayesian computation is all about. And if you read papers, you'll read about Markov chain, Monte Carlo methods or MCMC techniques, right? And this is where you can be extremely flexible with your modeling. You don't have to stick to known distributions. You don't have to work with distributions that work together. You can do anything, as long as you can find a way, as long as you can find a way to draw a random sample from the posterior distribution, you can learn as much data. So that's kind of the idea here. You do not need those conjugate distributions all the time for it to work. You can use very efficient sampling schemes, which are available these days, implemented multiple software tools that can effectively achieve the same job. And why is this important? I think that this is also important from a theoretical standpoint, right? So is it always possible for us to assume that our priors will be normally distributed? Maybe let's start with an assumption. It could be that we know something from theory about our priors, which states that it's not a normal distribution, it's some kind of a different distribution. And what is that distribution that you really can't write a full form solutions should be just involved. Probably you can utilize other sample techniques to achieve the same.

Unknown Speaker  1:09:42
Okay, okay,

Unknown Speaker  1:09:44
so you can construct empirical distribution. So the idea with sampling over here is you're not going to get a single point estimate, but you're going to get a sample. For those of you are familiar with bootstrapping and other frequentist methods, we know that you sometimes get distribution of parameters, whatever that is that you're estimating. It's the same idea here. You just drive a sample of whatever distribution you're studying and use that empirical distribution to draw some conclusions earlier in Mark's presentation. That's kind of what you saw, right? You saw those graphs. They were basically graphing the empirical distribution of all the projects that estimated in that particular study. And the advantage of this empirical distribution is you can construct any probability that you want from it, and you can ask that question as an example. You can say, what's the probability that if I go to college, my returns are going to be higher. Those end up meaning that model, but because Bayesian methods can control for endogenous within that setup, it might be a pretty strict so that kind of stuff becomes very, very useful when you are doing this approach. So here's a simple example. Again, things that you might see in other papers, etc. In this particular setup, we've got the posterior distribution of Y given the parameters to be an asymmetric, complex distribution in frequentist terms. This is very similar to quantile regression. That's the idea here. So it's the same as the Bayesian version of quantile regression. So you have three things that go into that particular distribution. You've got your data, which is the x prime, you've got your beta, which are the parameters that you're estimating, and you've got the P, which is also the quantiles that's basically being set up here, of course, that space that's not necessarily making one example, so you don't see any prior distributions on that. Now, on the left hand side here, this is a typical graph that you will kind of see a lot of data studies. And what this is showing on the on the x axis here is basically the number of draws, right? So this is how the simulated output is going to look like. This is the number of draws and the corresponding value for the beta for each draw. And you'll notice, for the first 150 or maybe not 200 you will see that this is an upward, floating line. What that's telling you is that in this particular estimation, the algorithm is searching for an optimal solution. In this case, this you'll see that the optimal solution which simulation model so the set of 1.5 it identifies that and then stays on track. So in a typical Bayesian study, you ignore, let's say, the first 200 observations. We call this, right? So the brain is a part which is ignored from constructing the empirical distribution, because that's a part where the algorithm is learning, and you want to achieve that convergence before you start interpreting that information. So here very simple things that you will see with almost any software application. This is something that can be easily applied, and it's something we should probably look at. You do not want to be using the first few elements

Unknown Speaker  1:13:03
or, yeah, so

Unknown Speaker  1:13:08
you don't want to be using that first few iterations for drawing. So this is pretty standard things that you will kind of look at. So the general workflow that you want to think of when you are construction study is specify a model for your data, right? So what is the model that's driving this? And one huge advantage of Bayesian studies is you can specify very complex models, models where you really can't use traditional frequentist. Sorry, I shouldn't use the word traditional frequentist approaches to kind of solve the problem. So you kind of specify, and then the key would be, how you set priors? What? How do you find a way to sample from the posterior distribution? And you summarize. Now let's the first step again, specify the model. In this case, I just used a very simple linear regression as my data generating process. It's a very simple linear regression setup over here. There are reasons why you might want to do it. People might say, Why do you want to do this for a Bayesian framework? Why not just do it in the frequency world? Sure, but the Bayesian framework, as we discussed earlier, offers a lot of advantages, maybe, maybe not. But as I said before, the biggest advantage is, what if I want to do a slightly different, more complex model? So one of the papers that I'm working in, working with with Jeff, is on, who's a co author of rank. It's basically, I'm trying to estimate sequential games based on game theory. There are sequential gains. And there are some techniques that have been discussed in the frequent as well, where you can kind of find solutions, but not necessarily the most efficient solutions. So we are working on Bayesian methods to kind of solve that. That data generating process is going to look very different from what we have over here. But it's still the start of the idea, right? So what is your model based on theory, and that's what's going to drive this, and you need to have very clear understandings and be able to justify this as well in terms of why you chose certain assumptions in this particular case. What about the frequentist world? Are we there without assumptions? The answer is no, but you don't ask for assumptions at that point. Lots of assumptions the frequentist world as well, and those are suitable, but in the Bayesian framework, what I've learned, at least from the publication process is they want you to be far more explicit about your choices. Something to think about. Justifying your prior is the second most important thing that's come come out. So the past, especially with review, with reviewers, we've had to use alternative prior specifications and use some form of model selection test, as David talked about earlier. So using base factors and other things, kind of look at what is the most efficient approach that you've been looking at. So in many cases, we've been asked to use a very flat prior, or uninformative prior set Andres talked about earlier as well, and that's because you want to reduce the influence of the prior as much as possible. But we may be missing something crucial there in terms of past knowledge. Why that's the case and ask. But impact presentations here, the next thing is going to be sample from the posterior distribution. There are many, many different ways in which you can do the sampling approach. The most popular techniques among the older techniques that we've seen are the metropolis Hastings algorithm and Git sampler. But apart from that, these days you've got so many other approaches like sampling. You might see Hamiltonian Monte Carlo, etc, etc, etc, very efficient tools in order to draw, in order to draw, from a posterior distribution. The code here is basically from open source R software. But as I was talking to Mark just before our presentation, I should have thought about 30 maybe even demonstrated that. But I said, let's look at the code copilot that chatgpt has. And by default we use Python, and you have pi MC, which is a Mr. Markov chain Montana simulator package, and I tried a couple of examples, and it was very different in generating code with that. So programming, which was a big learning for me when I was a graduate student, it took me a while to get to speak with that, even though I don't think I'm really that good at it, but there are tools out there that can help you do this job much easier. It won't help you to do this. You still need to do this part, and this is where I think we're good at because this is what we do, right? I mean in terms of our theoretical knowledge, etc, this is the part that we have to do, but this part, which was kind of probably my biggest stumbling block, and I was trying to use this early on my career is a lot more simple these days. We'll talk about other tools as well. But it is out there that it's easier to run. It's not as difficult as it looks, of course. And then we have our summary, summary of the posterior distribution, which, again, we've had the we've discussed that before in the past. Over here, Mark talked about some examples from his work as well. Okay, so let's talk a little bit about software now. And this is where I think it becomes interesting, right? We kind of use this term fixed versus flexible tools out there. And the reason why we do that is there are certain tools that are kind of a standard that we use in our research. We use that all the time, right? So we see that in multiple papers. We kind of use that a lot, but custom models, which I think are really, really interesting, is probably an area where Bayesian approach is really shining, because it's kind of really hard to find other ways to kind of follow it, hierarchical, the linear models, multinomial, endogenous regressors, as I said before, estimating games and different kinds of tools that we have. So those custom models, I think, is an approach, or if your theory needs a custom model, maybe it does. If it does, then Bayesian approaches give you a flexible approach to actually implementing it, something to think about. So also, this translates into the software tools. And please note that when I say fix and canned routines. I don't mean that there just can't plug and play, right? What I mean is you have easier approaches to get started without having to write a full set of codes. You want to run a Bayesian linear regression, go to stata and very easily do it. Stata is a tool that I was taught extensively in my lab training. So it's much more comfortable using I was much more comfortable using that initially, at least for me. So from that perspective, data and said, both have extremely powerful tools. And stata 18 correct me if I'm wrong, best of I think also provides an opportunity to write your own functions. I think the current version of Stata is moving much more towards the flexible tool set. So there's lots of advanced models, plus the fact that customized covid as well. On the other hand, you have r, you've got C, Python, and as I said, I don't even track GP default somewhere in that, but you can easily write code in that for very flexible quantum approaches are also available in between is this so called bugs, wind bugs, I mean, Jags and wind bugs 20 years ago, when I was doing Bayesian stuff, that was a that was kind of a tool to use, because for those who really couldn't do a lot of the high level coding, and we've used it extensively, I'm not so sure that it's that popular these days, but it's still out there. And obviously there's a lot that we can do

Unknown Speaker  1:21:07
using

Unknown Speaker  1:21:08
fixed routines. And if you're new to this, that's probably the best way to start, because then you can kind of really learn how to use it and move forward plenty of tools out there. So instead of it's pretty simple, you just kind of add days. I mean, they're often most of you, at least, should be familiar with the basic Regression command. As long as you have a base suffix in there, you can kind of run pretty much most of the general tools that you can use, plus some advanced so I strongly recommend if you use theta. Theta attainment is the latest by Mount Ron. So definitely see that that's got lot more tools being closed, available for you to use. So here are a couple of examples along in this in the same fixed versus flexible modeling approaches. Left hand side is a paper that I have that I was a co author on. It's about using Bayesian model averaging in the context of entrepreneurship research. And we found that, I mean, this is a long lens of what David talked about in terms of base factors and comparing different models, but our thinking was, you know, in this literature, there's especially for economic determinant, entrepreneurship, There's just plenty of papers where you've identified various factors and things. So what we thought was, which of these are most robust, and we don't, we deal with a specific problem of specification uncertainty, traditional regression models or frequentist models, can deal with samples, p values, take care of that. But what if I change my specification? Then it is possible that results you observe will actually flip or sometimes disappear? Goes back to a very important point that Andres talked about, which was a lack of replication, right? So we don't have replication, and therefore I can choose what to present. As long as I find statistics and infinite results, it's good. What if I change that model? What if I add an extra variable? How many variables can you add in a given paper? So we talk about all of those issues in this particular case. On the other hand, for more of a flexible approach, this is a cam routine. I mean, you can kind of use VMA is a very well established tool, and both in R and in STATA now routines that do this job for you very quickly and very efficiently. So there's a paper that was published with J Barney, Angie and Jeff Johnson, they talk about corporate diversification and a Bayesian approach. What they did in that paper was really tackle a very specific problem of endogeneity. And you might say, well, endogeneity is a very well established big literature. They distinguish, we've got multiple papers on the topic. They distinguish between what we call as intercept endogeneity, which can be corrected with traditional tools like selection approaches and instrument variables and other things, with slope endogeneity, which is a little bit more involved, a much more difficult problem to solve. There are some frequentist approaches, if we deal with it, but it's not as easy to implement. So they derive a Bayesian version of that in this video, and then they talk about what that actually influences the outcome, in this case, going from fixed routines to more flexible approaches, all of them offer value in terms of what they can learn about our own field. So you should kind of start thinking about if you're interested in this. Finally, I just wanted to briefly touch upon the scan environment, which is also a really highly powerful environment for Bayesian computations used by a lot of different people. In fact, I know that Angie here contributes to that community a lot, so I think she might be talking about it later in her presentation. So this is also another opportunity for for you to kind of stop thinking about alternative ways in which you can do the programming part the software. So there's lots of literature out there about Sam, and it's also, I personally have not used Sam much for my work, but I said I'm really more old school. I sort of say that are for most of the computational work that I have, I'm done, but I'm pretty sure we'll talk more about this. Thank you very much. I'm

Unknown Speaker  1:25:54
gonna do a quick change, because we're running late in time, and usually the emphasis with regard to the publication side. Of this has been very publication further. So I suggest we hear Mark first on the publication side and then leader. Have one big what time left with regard to question and answers? So I'll just hand over to you right now. Yes. Sorry. And Marcos present has also the most, I think, the longest experience for the publishing papers, so he had thrown out the first ones in strategic management. Thank you. Okay.

Unknown Speaker  1:26:40
I did this in, in a way, just a continuation of what I said earlier. If you look at this second bullet point here, found this out the hard way. No, my experience was nobody wants to be the one to allow somebody to come along and say, Hey, what we've been doing for years and years doesn't really make that much sense. I'm calling into questions so much important that has been so well established. You've got to be careful about that and and here's my my study comment, as true as it may be, don't do

Unknown Speaker  1:27:26
it. You may be right as rain, but you're never going to get anywhere. You start out that way,

Unknown Speaker  1:27:33
and so we you have to be very careful in our frame so that it's not a direct assault on frequent of statistics and the use of treatment of statistics. And in the name of full disclosure, speaking to my statistician friend and showing him what we do in management, he made me pretty because he pointed out all the things that we're doing in a program, playing fast and loose with a lot of

Unknown Speaker  1:28:16
statistical methodology. Now, on this point here we have, we seem to have this strong desire for hypothesis testing into

Unknown Speaker  1:28:28
entry. You can still do that if you want to amazing. It looks a little different. And in my view, done well it it's much with a frequent approach, primarily because of the output that posterior distribution, where you're showing them the probabilities of this effect on that outcome. And for me, I think this is one of the most compelling things about a well done based that article from high Angie J Hardy and Jeff Dodson, that is a great example of this very difference, the problem that they were addressing theoretically is a very Pelican

Unknown Speaker  1:29:39
fit. Here are some other problems that you run into, and does that have to do with the fact that statistically, if we're using a previous approach, we need to have a random sample from a population, and if or actually sink and truly brand and sample from a population. Does anybody.


Unknown Speaker  0:00  
Be generalized

Speaker 1  0:04  
the issues of priors here. This is a big deal. It's been talked about here today, so I won't go over all of that again. We'll just point out that in people's minds, priors are are such a challenge because of the belief that some priors that in many cases are often mistaken, but in fact, those priors may be some of the most compelling characteristics of a reasonable approach, especially when we're updating prior to we've done some analysis. We have an idea about what that distribution looks like.

Unknown Speaker  0:53  
Analysis,

Speaker 1  0:57  
increasingly, increasingly. Okay. This, this first bullet point here, that's something that that's feedback I received every time I tried. People want to see that comparison. Again, you give them a candid, accurate comparison. And so that's

Unknown Speaker  1:21  
something

Speaker 1  1:26  
that's very difficult to manage, but if you can just show that, I would say your initial submission, if you show a

Speaker 1  1:38  
comparison, that's good practice. Maybe that will always be the case, but in my experiences, to show the things that we were talking about here today, and then again, if you're testing on hypotheses, here's the data, here's the analysis that we did, here's the here's the analysis that we did. Here's the posterior distribution. This is what this tells us. We believe this supports our economy. And the great thing about that approach is we're not relying on some arbitrary P value. We're just relying on that probability people can draw their conclusion, and that's really what it's all

Speaker 1  2:35  
about. Now I have here several slides of things that you've seen before. I'm just talking through these to make the point that here, here's one for an individual firm and what it looks like for them. This is one that's had a wide range of experience with new product or new market entry, centered in a negative territory. They do it a lot. Sometimes it has worked out well, probability is negative, but there's still a chance to be positive. And that's something that, again, I think, is so compelling about these so real world doesn't work every time. Methodology works, but if we're looking at an action in the firm case, we wouldn't expect that it works fabulously every time. Just makes sense. There ought to be some distribution, and we can demonstrate what that is now here, I'm just showing you this to make the point in closing here that you can develop a table like this is a certain same set of data. I've shown you the distributions for this. Here's a table, if reviewers on a table so they can make a comparison. What do you get with Bayesian What do you get?

Unknown Speaker  4:07  
Frequent approach is easily doable. Okay, all right. Well, thank you for your your attends here today, your questions. Look forward

Unknown Speaker  4:21  
to any questions.

Speaker 1  4:24  
And back to this point. I mentioned this before. This is used in a lot of places. I think it's going to increase its use. And down the road, we won't face some of these challenges. We won't have to offer a comparison will

Unknown Speaker  4:47  
be more. I applaud you for being here and having these give it a try. I.

Unknown Speaker  5:00  
Hi everyone.

Unknown Speaker  5:07  
I wish to be a little more

Speaker 2  5:12  
engaging throughout the session. I want to introduce how I became gorgeous. Became.

Speaker 2  5:30  
And I would like to introduce like, why and how many school reasons are publicly very contrarian, meaning that there are not enough resources for us to learn. But the other good side is it's similar in Russian learning as well in Russia, and is once known to be contrarian, and we became very optimistic. Became very optimistic, that the good side of criteria is not people remaining are very hardcore beings. So if you ask them, which I, when I first learned, I asked for them a lot of help when they're moving time and reply. So I really urge you to, if you want to learn, a reach out to me or this professors to ask where I want to learn this and where I should start that being said, I have five minutes to introduce how I learned. And so I think on the website I used to introduce myself as my movie unity. And what I mean by that is very my life, of how I operate like just from when I was listening to numbers, what you're interested in, to require and

Speaker 2  7:12  
and the reason I think that we should be able to be a community is building up. Mentioned that the code library can provide right now, even the Boyle building, we can build on the previous models. So when I did the research on i The first thing I did was I searched what case studies out there

Unknown Speaker  7:41  
within technical communities and compared like how they covid testimonial and the way I ended up shooting the hierarchical spine in order to cortex. That was

Speaker 2  7:56  
because there was a very great case. I didn't start programming those. I feel combined and also related to basin model enriching, because, like, if you have the same research design, you can wait

Speaker 2  8:32  
I mean, just please write me a mail, a m google and I consulted three of my friends and helped them learn media. So sort of wrap up, I think learning media is learning a method to learn so that your research and your theory and your observation your life become very consistent. That was my experience I had, and that is why I think last year I'm sitting on your seats, and this guy got promoted, and I just wanted to deliver.

Speaker 3  9:19  
And I would add to that, that most of place in scientific communities are not accomplished by single people like Einstein and geniuses. These are collective efforts, and they're really effective as collective efforts. And that shows there are communities out there, and we're trying to build community with that, because together, we can walk that path. So that's what in these contributions really helpful. And doctoral students play a big role with regard to how our methods really change, because their generational changes so very short, brief, I'm coming to my five minutes. Why haven't we seen more Asian analysis in the field already in the past, many. That was recent decades. You had to do very complex math during do to do the calculations, and most people can't do them. I can't do them. That has completely changed, as you pointed out, now with that computational power, just drive us. So today it means machines can run things that 10 years ago, you would have had the mainframe computer to run on for two hours, and now it's 30 seconds on your machine. That's partially the power of the machine. So Silicon Valley, that's also they're improving the estimation algorithms behind it. They're just getting constantly more effective. And these two things together and open doors. So that is really and if you're not familiar with it, there are versions out there is a good starting point for flexibility. So there's an entire tool set out there and documentation and support that you can find. So that's the one side on the technical execution of it. Second element is training. Yes, it's a more flexible tool. It's a more complex tool. You have to make more decisions, and you got to justify the decisions. So we need training. But there's also sources out there. They may not be directly available, like the frequently statistics books, but they are as I like, for example, really Richard Albert book on statistical rethinking, because he also has a lecture series that's available for free, and there is our code that you can use. So there is stuff that you really can play with. So that's one side. Other side is there are courses at most universities now look around. They may not be in the management area, they may be in the stacks, but they may also be in marketing. They may be in psychology, so they are out there and nearly all universities. And then sign collaborators are not that many yet, but at the same time, this creates the momentum. So working for Mark, working with Jeff Dodson, was very, very helpful. I know working with Jeff Dodson, they called him up and I have questions. So again, as a community, we can push it, and we don't need to have all the same skills. We can draw on each other's skills from a patient. There's not much Bayesian on the program, but there is another session on Sunday that I want to point out, and that is Tom Stata, who has a representative here. I'm also a say that user, but I'm not paid. There is no other they're the only ones here with regard to providing additional training, with regard to Introduction to their system. Their system is works for me, but at the same time, SPSS more than what you actually mentioned on there. All of the big companies now have some Bayesian statistics built into their standardized approach. So summary, Bayesian analysis can be applied to virtually any research question that we have and give us an answer that actually more powerful than the answers we're getting currently, we can use prior distributions that implies an opportunity to build on what we already know and integrate that in there. And we can probe How much more does it make a difference? We can get a quantified estimation of that. We can use that as a discussion. Advanced computers allow us to do this quickly, so that we're not having to wait for results for a week or something. This is on your desktop. On your laptop, you can run page, no problem. Publication of Bayesian studies pop journals is feasible. We see people do it. There is more of an openness about it as the interest and the community grows. There is still challenges, as Mark points out, but these are challenges worth taking on, because I think they change the field. They also get your attention. I like Brent Goldfarb statement, if you're a PhD student or junior faculty now, we better start to future proof our methods training and method capability, because the world as I and others, is going to look very different methodologically 10 years from now. And very, very convinced of that, and being in a first starting cohort is an opportunity versus three or five. So what we need right now, I think, is really willingness of management scholars to become familiar with a different but powerful statistical tool, and doing that as a community, that's where I conclude here. And we finally are at an opportunity with regard to you asking some questions, so I'll open up the floor.

Unknown Speaker  14:40  
A sort of basing religion, right? It seems like almost urban suffer from

Unknown Speaker  14:49  
fundamentals. Are

Speaker 4  14:55  
there? You? Are there situations where

Unknown Speaker  15:05  
you use traditional statistics?

Speaker 1  15:16  
Well, for me, first to the first part of your question, I have reviewed papers they use frequentist statistics, and if I spot some of the things that they're playing fast and loose with the rules, I'll point that out. I won't ding the whole paper for it, but I will definitely point that out and to your the second part of your question. Well, what was the second part? Oh, yeah. Would we have yeah for some question depending on the question that I was looking at? Sure, maybe

Unknown Speaker  16:00  
sometimes traditional statistics.

Speaker 1  16:06  
I don't know if I'd make that distinction, but if it's a straightforward thing, for example, where I really do have a random sample, sure I wouldn't be opposed to it, I would say

Speaker 3  16:17  
one of my experiences was, if you send in a paper or analogy, was, if you send in a paper right now that doesn't have any stars in it, you might get a rejection that is just saying, like you don't even know what the plane is playing. So I still report p values, but I don't use them to interpret my results. I use them to be perfect. For example, I would actually encourage you what you've heard here, we are much more powerful as reviewers than we are as authors, because as reviewers, you can ask all kinds of questions with regard to it. You are behind a veil of anonymity, but you're still pushing the field in a certain direction. But asking an author who said, like, oh, forget all about that now run a patient analysis, I think that's over. We have to recognize our field is still using this mainstream and as a review, I have to be compliant with that, but I can push it into other things. It's not just Bayesian. I can ask the tech sizes. I can say, like you're all for interpreting this, probably not a probability, so give me something

Speaker 5  17:17  
else. Oh, quick comment. Andrew elements blog is a very popular learning Bayesian place, and he has a blog about Bayesians are frequentists. So depending on how you design your data space and sample space, they are equivalent to each other, in fact. But sometimes it's much faster to use a frequentist. Sometimes,

Speaker 3  17:38  
I think we're all suffering also from the side, I get more there are so few people who can read through a daily paper. I get so many requests on pages. I turn the other ones down by my choice that I rather want to promote some patients. But it's not there that's another element that happens in this process. Good. There was another question. Because

Speaker 6  18:01  
it, I have a question, because the ability to draw appearances on for very specific entity like the team or firm or whatever, wouldn't that make maybe an interference like a perfect tool to investigate moderation? Because if I have a moderation like,

Speaker 6  18:27  
convenient to me, but oftentimes, like, clear, like, how exactly that? Exactly like, Are there actually data on certain reasons that I love being if I had my under team and I proposed motivation based on a certain characteristic within the team, I could like draw serial

Unknown Speaker  18:51  
distributions

Speaker 6  18:52  
for every team and then sort them based on the moderation value, then investigate how that shifts would that

Unknown Speaker  19:09  
definitely be able to do that in the Bayesian framework

Speaker 3  19:15  
doesn't allow that. I think I would point out you really have a power problem most of our interactions, we have so much noise, and the noise the effect on the effect is exponential because of the uplift of the term that produces. So again, I see Bayesian Boys in the Boat, where you can dig deeper, you can test some assumptions by something out. And that might give you a deeper insight of how the moderation really what moderation pattern you kind of see, and there's valuing that person's just a judgment, is there a moderation and just stuff your analysis. So it's the flexibility in that and moderation analysis in completely feasible.

Speaker 7  19:59  
So. Technically, someone who really wants you to do frequently, because, of course, they might ask you to do a team fixed effects or something like that, create dummy variables based on your conditions and then do interaction and then analyze the moderation, as Angie has just pointed out, you're going to have power problems if you don't have sample in that case. So

Speaker 3  20:21  
I think what Mark earlier with the nesting, you can nest this with regard to individual variables within this organization, and is relatively easy then to account for the different effects interactively, with regard to giving the best estimate. That's much more powerful than our usual approach with regard to digging into

Speaker 8  20:42  
that. Question

Unknown Speaker  20:55  
about priorities when of search, is that still markets, or

Unknown Speaker  21:22  
like all are like, or

Speaker 3  21:32  
sometimes my take on what matters. If you don't know what's in your data, your favorite is alternates, assumption of normal distribution and all these things, and they're not going away here. I have the flexibility and the transparency that I'm actually running this with that prior. So if I don't know anything, I can run a flat tire. And hopefully your research is going to tell you something. And maybe the next study you do, or the next study somebody else does, will accumulate so that we get to a point of having some confidence that that priors. Or I would say the same thing. I would tell you, for the frequentist side, get a larger sample. With the frequentist, I can guarantee you eventually some star will pop up. But here, with the larger sample, I just get data dominance. But you meet having the data dominance still no effect. You just recognize it's not the prior that's sliding. All of these are flexible but much more powerful tools to understand what is really in my data. And one of the biggest sad things about statistic, there's nothing wrong with think of this statistic. There's nothing wrong with repeat value. It's just there's so much information in the data that we're throwing away. It took us so much time to collect this, and then we're not really going into understanding the effect. But rather, are satisfied with the dichotomous statement on a point five arbitrary value. It looks like we have an effect. Great. But talk to managers, if Mark's example, if you talk managers that restructuring is helpful, you only can tell them it's helpful. What are they going to ask you next? You're going to say, like, how helpful. What's the probability that I get a million or 2 million? What's the probability of me losing money with a P value? We can say, like, I can't tell you that. I can tell you zero isn't in there. 95 for probability when I do random sampling. But that's not actionable knowledge with frequency algorithm, posterior distribution, I can tell you exactly more than a million. What's the probability of that? More than 5 million, then I can ever quantify data driven answer to it. That's power.

Speaker 9  23:45  
Thank you very much. I'm still fairly new to management research and management field in general, and so my question is, is there a specific entity or stores to go to as far as reporting standards for writing

Speaker 3  24:07  
what kind of standards, what templates to use, what's the same template we always have look at marks or other people that are published in order to understand how is that journal doing this? And these are the current norms. Are they always fitting very well? No, they're not. And then you have to deviate and kind of find your way through. So in that sense, there isn't a simple answer with regard to what but there is answers. There is other textbooks that also select this is the information that is needed in order to properly describe what you did. And I

Speaker 7  24:40  
think there's also a very recent paper, probably just accepted as coming out in Oxford Handbook, 100% for Jeff Dodson and Tyson Tai Mackey. They wrote a simple paper on a primer on losing houses and 100% results and

Speaker 3  24:56  
everything. So we can just sit on the research guide. Yeah, that would be another. Think

Speaker 7  25:00  
it's accepted by much further out as yet. But keep in

Speaker 3  25:04  
mind, this is a real problem for our field in general. Our method sections are too short. We have a long review of running nodes in their vehicle paper, and then we can't replicate you that study, because there's not there so many information missing with regard to what they actually did. Now here now we want to use another method that people don't understand it. You will have to explain more, and you will have to do it in a good way in order to get through the gates with regard to getting it published. And that's just the nature by doing something novel, but that's why you signed up for science, right? For doing novel stuff that is challenging to people, not to just follow the general herd, right?

Speaker 1  25:41  
I would take a look in marketing journals and see what what they're doing. Yeah,

Speaker 3  25:50  
marketing is a very close area. And I can also say in our marketing department, Iowa State, we have Asian statisticians, Asian trained people in that department, and they are close with regard to studying organizations. There's a lot of similarities and good learning

Speaker 5  26:07  
opportunities regarding the previous question. Could you remind your name? Yeah, Yuri, yeah, regarding Yuri's question, and also Andreas, just comment right now. I think scholar as a what, first year PhD student, I think scholars work is very entrepreneurial, meaning that we have a hypothesis that we want, and we are collecting data to persuade the reviewers. And this is exactly what the entrepreneurs is trying to do. They want to show that their contrarian belief is correct, and they collect evidence. And the framework of Bayesian is very beneficial to that, because you can learn from yesterday's you based on my belief so far accumulated, what is the additional data that I can more collect in order to justify this hypothesis? So there is a Bayesian persuasion work related to that, if you are interested anything

Speaker 3  27:01  
on the positive news. Note with that too. If you hear the stories from the marketing field, what they're going to tell you that they went through the same thing we went. There was a time where there was a very smooth on Beijing people. It was hard to publish. And then they described there was a watershed event. Suddenly, all the journals wanted also some Bayesian studies. And publishing Bayesian studies actually got to some degree, they argue too easy. You have to get on the brakes. And it's also quality patient work that really gets in. But so then that will set your face uphill. You really also see the downhill right, and you bring in a front group if you get into these kind of areas. So it's an up and a downside, and I see lots of potentials down the road with regard to this, because this is so established, this is so powerful, this will not dominate necessary. I don't want it necessarily become dominated as statistical significance testing, but it will become part of our tools. Now that I would say like thanks because they also wanted

Unknown Speaker  28:01  
to kick us out of

Unknown Speaker  28:13  
your Thank you. Thank

Unknown Speaker  28:14  
you. Thanks

Unknown Speaker  28:15  
for your flexibility.

Unknown Speaker  28:16  
Yeah, of course, yeah.

Speaker 10  28:21  
Just One question. Quick

Speaker 6  28:26  
Message, yeah,

Transcribed by https://otter.ai
